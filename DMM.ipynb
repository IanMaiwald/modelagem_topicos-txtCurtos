{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b551da3-e71a-4200-89ed-a1cd24aa0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fcb254-c93a-4172-a723-4b118229f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM:\n",
    "    def __init__(self, n_topics, n_iter, alpha, beta):\n",
    "        self.n_topics = n_topics\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Vetorize o texto usando o CountVectorizer\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(data)\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        self.vocab = np.array(vocab)\n",
    "\n",
    "        # Inicialize o modelo\n",
    "        self._initialize(X)\n",
    "\n",
    "        # Atualize o modelo iterativamente\n",
    "        for _ in range(self.n_iter):\n",
    "            self._iteration(X)\n",
    "\n",
    "    def _initialize(self, X):\n",
    "        n_docs, n_words = X.shape\n",
    "\n",
    "        # Inicializa o tópico de cada documento\n",
    "        self.doc_topics = np.random.randint(0, self.n_topics, size=n_docs)\n",
    "\n",
    "        # Inicializa contagens de palavras por tópico e documentos\n",
    "        self.word_counts_by_topic = np.zeros((self.n_topics, n_words))\n",
    "        self.doc_counts_by_topic = np.zeros(self.n_topics)\n",
    "\n",
    "        # Atualiza contagens de palavras por tópico e documentos com base nas atribuições iniciais do tópico\n",
    "        for d in range(n_docs):\n",
    "            topic = self.doc_topics[d]\n",
    "            self.doc_counts_by_topic[topic] += 1\n",
    "            self.word_counts_by_topic[topic] += X[d].toarray()[0]\n",
    "\n",
    "        # Inicializa a matriz de probabilidades condicionais\n",
    "        self.conditional_probs = np.zeros((n_docs, self.n_topics))\n",
    "\n",
    "    def _iteration(self, X):\n",
    "        n_docs, n_words = X.shape\n",
    "\n",
    "        # Percorra todos os documentos\n",
    "        for d in range(n_docs):\n",
    "            # Remova o documento atual das contagens\n",
    "            current_topic = self.doc_topics[d]\n",
    "            self.doc_counts_by_topic[current_topic] -= 1\n",
    "            self.word_counts_by_topic[current_topic] -= X[d].toarray()[0]\n",
    "\n",
    "            # Calcule as probabilidades condicionais para cada tópico\n",
    "            for k in range(self.n_topics):\n",
    "                word_likelihood = np.sum(np.log(self.beta + self.word_counts_by_topic[k]) * X[d].toarray()[0])\n",
    "                self.conditional_probs[d, k] = np.log(self.alpha + self.doc_counts_by_topic[k]) + word_likelihood\n",
    "\n",
    "            # Normalize as probabilidades condicionais\n",
    "            self.conditional_probs[d] = self.conditional_probs[d] - np.max(self.conditional_probs[d])\n",
    "            self.conditional_probs[d] = np.exp(self.conditional_probs[d])\n",
    "            self.conditional_probs[d] = self.conditional_probs[d] / np.sum(self.conditional_probs[d])\n",
    "\n",
    "            # Atribua o documento a um novo tópico com base nas probabilidades condicionais atualizadas\n",
    "            new_topic = np.random.choice(self.n_topics, p=self.conditional_probs[d])\n",
    "            self.doc_topics[d] = new_topic\n",
    "\n",
    "            # Atualize as contagens de palavras por tópico e documentos com base no novo tópico\n",
    "            self.doc_counts_by_topic[new_topic] += 1\n",
    "            self.word_counts_by_topic[new_topic] += X[d].toarray()[0]\n",
    "\n",
    "    def _iteration_batch(self, X, batch_indices):\n",
    "        for d in batch_indices:\n",
    "            self._iteration(X, d)\n",
    "\n",
    "    def fit(self, X, vocab, batch_size=1000, n_jobs=4):\n",
    "        self.vocab = vocab\n",
    "        self._initialize(X)\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            print(f\"Iteration {i + 1}/{self.n_iter}\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "                n_docs = X.shape[0]\n",
    "                batches = [(i * batch_size, min((i + 1) * batch_size, n_docs)) for i in range((n_docs + batch_size - 1) // batch_size)]\n",
    "                for batch_start, batch_end in batches:\n",
    "                    executor.submit(self._iteration_batch, X, range(batch_start, batch_end))\n",
    "\n",
    "    def get_topics(self, n_words):\n",
    "        topics = []\n",
    "\n",
    "        # Calcule a matriz de distribuição de palavras por tópico\n",
    "        word_distribution_by_topic = (self.word_counts_by_topic + self.beta) / (np.sum(self.word_counts_by_topic, axis=1)[:, np.newaxis] + self.beta * self.word_counts_by_topic.shape[1])\n",
    "\n",
    "        # Para cada tópico, encontre as palavras mais relevantes\n",
    "        for k in range(self.n_topics):\n",
    "            top_word_indices = word_distribution_by_topic[k].argsort()[-n_words:][::-1]\n",
    "            top_words = self.vocab[top_word_indices]\n",
    "            topics.append(top_words)\n",
    "\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f503d9d-2d41-4158-9f9c-5b464b00b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "#df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1179d5b-da28-468d-8b95-a82651e8b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetoriza os documentos \n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "X = vectorizer.fit_transform(df['texto'])\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a7779-8ab1-4866-8fb7-48e5350488f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie e ajuste o modelo DMM com processamento em lote e paralelização\n",
    "dmm = DMM(n_topics=20, n_iter=50, alpha=0.1, beta=0.01)\n",
    "dmm.fit(X, vocab, batch_size=1000, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70bb95e-585b-4972-b2c8-93d18c9f0641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha os tópicos\n",
    "topics = dmm.get_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee852b1e-2d74-423a-be8c-0ce6fbb2ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topico {i}: {' '.join(topic)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
