{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6016f236-b022-4855-b311-9ac521cbe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d12ca2e1-2340-451c-9d72-d3f55797c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_polya_urn(num_colors, initial_counts, iterations, poisson_lambda):\n",
    "    urn_counts = np.array(initial_counts, dtype=int)\n",
    "    for _ in range(iterations):\n",
    "        chosen_color = np.random.choice(num_colors, p=urn_counts / urn_counts.sum())\n",
    "        additional_balls = np.random.poisson(poisson_lambda)\n",
    "        urn_counts[chosen_color] += additional_balls\n",
    "\n",
    "    return urn_counts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d7067d-24b6-4aaa-a169-a85be00d7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_mixture_model(tokenized_docs, num_topics, alpha, poisson_lambda, iterations):\n",
    "    num_docs = len(tokenized_docs)\n",
    "    vocab = sorted(list(set(word for doc in tokenized_docs for word in doc)))\n",
    "    num_words = len(vocab)\n",
    "    word_indices = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    word_counts_by_topic = np.zeros((num_topics, num_words), dtype=int)\n",
    "    topic_assignments = np.random.randint(0, num_topics, size=num_docs)\n",
    "    epsilon = 1e-9  # Small value to avoid division by zero\n",
    "\n",
    "    # Initialize word_counts_by_topic\n",
    "    for doc_idx, doc in enumerate(tokenized_docs):\n",
    "        topic = topic_assignments[doc_idx]\n",
    "        for word in doc:\n",
    "            word_idx = word_indices[word]\n",
    "            word_counts_by_topic[topic, word_idx] += 1\n",
    "\n",
    "    # Main loop\n",
    "    for _ in range(iterations):\n",
    "        for doc_idx, doc in enumerate(tokenized_docs):\n",
    "            current_topic = topic_assignments[doc_idx]\n",
    "\n",
    "            # Remove word counts of the current document from word_counts_by_topic\n",
    "            for word in doc:\n",
    "                word_idx = word_indices[word]\n",
    "                word_counts_by_topic[current_topic, word_idx] -= 1\n",
    "\n",
    "            # Calculate topic probabilities\n",
    "            doc_word_indices = [word_indices[word] for word in doc]\n",
    "            doc_word_counts = word_counts_by_topic[:, doc_word_indices]\n",
    "            topic_probabilities = (doc_word_counts.sum(axis=1) + alpha) / (doc_word_counts.sum() + num_topics * alpha)\n",
    "            topic_probabilities *= (doc_word_counts + poisson_lambda).prod(axis=1)\n",
    "            topic_probabilities = np.maximum(topic_probabilities, epsilon)  # Ensure non-negative probabilities\n",
    "            topic_probabilities /= topic_probabilities.sum()  # Normalize probabilities\n",
    "\n",
    "            # Sample a new topic and update word_counts_by_topic\n",
    "            new_topic = np.random.choice(num_topics, p=topic_probabilities)\n",
    "            topic_assignments[doc_idx] = new_topic\n",
    "            for word in doc:\n",
    "                word_idx = word_indices[word]\n",
    "                word_counts_by_topic[new_topic, word_idx] += 1\n",
    "\n",
    "    return topic_assignments, vocab, word_counts_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cc1652-ce1f-4992-ac31-e84b241b841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data['tokens'] = data['texto'].apply(lambda x: [word for word in word_tokenize(x.lower()) if word.isalnum()])\n",
    "    return data['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d6ba1f-7302-4070-bd45-f98479abaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpu_pdmm(data, num_topics, alpha, poisson_lambda, iterations):\n",
    "    tokenized_docs = preprocess_data(data)\n",
    "    topic_assignments = dirichlet_mixture_model(tokenized_docs, num_topics, alpha, poisson_lambda, iterations)\n",
    "    return topic_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe38e581-73a8-462a-a153-dfb1048900db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_by_topic(word_counts_by_topic, vocab, num_top_words=10):\n",
    "    top_words_by_topic = []\n",
    "    for topic_idx in range(word_counts_by_topic.shape[0]):\n",
    "        top_word_indices = np.argsort(word_counts_by_topic[topic_idx, :])[-num_top_words:][::-1]\n",
    "        top_words = [vocab[word_idx] for word_idx in top_word_indices]\n",
    "        top_words_by_topic.append(top_words)\n",
    "    return top_words_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38793e2-b15e-4dda-8ec2-881765234b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronavirus aceitar braco abrir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>achar eleitor bolsonaro medo coronavirus febre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fome coronavirus entrar fila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trancar replies twetr sala coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caso coronavirus confirmar Brasil mundo querer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>preciso comando vermelho decretar toque recolh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>urgente reporter confirmar segundo morte coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>informativo elaborar equipe viano azevedo advo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>vivo Paulo confirmar primeiro morte covid19 Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>bolsonaro protesto coronavirus pegar gado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    texto\n",
       "0                         coronavirus aceitar braco abrir\n",
       "1       achar eleitor bolsonaro medo coronavirus febre...\n",
       "2                            fome coronavirus entrar fila\n",
       "3                  trancar replies twetr sala coronavirus\n",
       "4       caso coronavirus confirmar Brasil mundo querer...\n",
       "...                                                   ...\n",
       "99996   preciso comando vermelho decretar toque recolh...\n",
       "99997   urgente reporter confirmar segundo morte coron...\n",
       "99998   informativo elaborar equipe viano azevedo advo...\n",
       "99999   vivo Paulo confirmar primeiro morte covid19 Br...\n",
       "100000          bolsonaro protesto coronavirus pegar gado\n",
       "\n",
       "[100000 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9535496-01f9-4080-92c3-7f6761392a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize os documentos\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokenized_docs = [tokenize(doc) for doc in df['texto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6747397e-936e-4c98-aa56-5ced07a2198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute o algoritmo GPU-PDMM\n",
    "num_topics = 10\n",
    "alpha = 1.0\n",
    "poisson_lambda = 1.0\n",
    "iterations = 10\n",
    "topic_assignments, vocab, word_counts_by_topic = dirichlet_mixture_model(tokenized_docs, num_topics, alpha, poisson_lambda, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2d0c08-c392-4141-b7c4-6c97b8cceb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenha as palavras mais frequentes para cada tópico\n",
    "num_top_words = 10\n",
    "top_words_by_topic = get_top_words_by_topic(word_counts_by_topic, vocab, num_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d35320-e578-4104-95f7-b89e2f4e0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ksksks, espera, agitador, gtgtgtgtgtgtgtgt, feu, elio, dorinha, traum, sucedir, reclame\n",
      "Topic 2: ofc, urrar, probleminhar, amiguito, fodi, gaitar, opor, leoa, contorcer, carrer\n",
      "Topic 3: cierto, maozinhar, huh, adio, glorificar, passional, olhae, permaneca, adentro, filmaco\n",
      "Topic 4: coronavirus, covid19, caso, brasil, virus, pessoa, dia, falar, ficar, mundo\n",
      "Topic 5: injuriar, admira, arrebatar, demaiss, culpo, multiverso, mamatar, bolivar, osssse, acab\n",
      "Topic 6: seguim, asem, scrr, bostao, karalho, defeito, narnio, sio, ksksksk, papinho\n",
      "Topic 7: hipnotizar, encima, fester, alain, equip, desgraceirar, pratiquir, taz, ostir, coincir\n",
      "Topic 8: parlar, suplicy, perro, ahs, tofr, tags, apocalypse, vomito, brisar, desire\n",
      "Topic 9: vergo, cutir, correo, firulal, demaissss, tuit, uhum, nuke, surrealista, pacheco\n",
      "Topic 10: querier, fdx, estadisticas, tito, ramalho, providencie, volo, ruandar, cacha, loirinha\n"
     ]
    }
   ],
   "source": [
    "# Exiba os tópicos e suas palavras mais frequentes\n",
    "for topic_idx, top_words in enumerate(top_words_by_topic):\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8155b-a719-4e92-8d8f-bfe1414d3260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
