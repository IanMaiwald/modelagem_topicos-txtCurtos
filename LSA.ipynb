{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4bb195d-ec05-405c-a9b6-06c989aab82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim.matutils import kullback_leibler, jensen_shannon\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d740e4-702d-49e1-bc41-270ad7983386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronavirus aceitar braco abrir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>achar eleitor bolsonaro medo coronavirus febre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fome coronavirus entrar fila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trancar replies twetr sala coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caso coronavirus confirmar Brasil mundo querer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>preciso comando vermelho decretar toque recolh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>urgente reporter confirmar segundo morte coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>informativo elaborar equipe viano azevedo advo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>vivo Paulo confirmar primeiro morte covid19 Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>bolsonaro protesto coronavirus pegar gado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    texto\n",
       "0                         coronavirus aceitar braco abrir\n",
       "1       achar eleitor bolsonaro medo coronavirus febre...\n",
       "2                            fome coronavirus entrar fila\n",
       "3                  trancar replies twetr sala coronavirus\n",
       "4       caso coronavirus confirmar Brasil mundo querer...\n",
       "...                                                   ...\n",
       "99996   preciso comando vermelho decretar toque recolh...\n",
       "99997   urgente reporter confirmar segundo morte coron...\n",
       "99998   informativo elaborar equipe viano azevedo advo...\n",
       "99999   vivo Paulo confirmar primeiro morte covid19 Br...\n",
       "100000          bolsonaro protesto coronavirus pegar gado\n",
       "\n",
       "[100000 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92675ff7-7f20-4433-8e55-fd3211a57d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=None, \n",
    "                                    max_df=0.7, \n",
    "                                    min_df=5, \n",
    "                                    use_idf=True, \n",
    "                                    smooth_idf=True, \n",
    "                                    sublinear_tf=True)\n",
    "matriz_tfidf = tfidf_vectorizer.fit_transform(df['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de279928-eb0e-4cd2-ad04-d216921694ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo todos os documentos como uma lista\n",
    "docs = df['texto'].tolist()\n",
    "\n",
    "# Criando uma lista de palavras\n",
    "words = []\n",
    "for doc in docs:\n",
    "    words += doc.split()\n",
    "\n",
    "# Removendo palavras repetidas\n",
    "terms = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2f1595-6692-4495-a193-a92b7bf3ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim.matutils.Sparse2Corpus(matriz_tfidf.T)\n",
    "dictionary = gensim.corpora.Dictionary.from_corpus(corpus, id2word=dict((i, term) for i, term in enumerate(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46715c9-781c-4617-868c-832a9e313550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o corpus em conjuntos de treinamento e teste\n",
    "num_docs = matriz_tfidf.shape[0]\n",
    "train_docs = np.random.choice(num_docs, int(num_docs*0.8), replace=False)\n",
    "test_docs = np.setdiff1d(range(num_docs), train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ea21a5-a828-447a-a24e-942c0de327b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo LSA para o conjunto de treinamento\n",
    "num_topics = 10\n",
    "lsa_model = gensim.models.LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "train_corpus = gensim.matutils.Sparse2Corpus(matriz_tfidf[train_docs].T)\n",
    "lsa_model = gensim.models.LsiModel(train_corpus, id2word=dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009fe4e7-38d4-4fe4-972e-1a40a0a188fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruindo o corpus de teste a partir do modelo LSA\n",
    "test_corpus = gensim.matutils.Sparse2Corpus(matriz_tfidf[test_docs].T)\n",
    "reconstructed_corpus = []\n",
    "for doc in test_corpus:\n",
    "    reconstructed_doc = lsa_model[doc]\n",
    "    reconstructed_corpus.append(reconstructed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13825fe-8346-41b7-b55b-20499e7a95d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tópico 0:\n",
      "['0.785*\"abrigo\"', '+', '0.223*\"portr\"', '+', '0.204*\"iguaizinho\"', '+', '0.136*\"enviado\"', '+', '0.126*\"inhuman\"', '+', '0.126*\"bastioes\"', '+', '0.088*\"dgsaude\"', '+', '0.088*\"moratoria\"', '+', '0.087*\"daninha\"', '+', '0.085*\"caverno\"']\n",
      "Tópico 1:\n",
      "['0.778*\"portr\"', '+', '-0.434*\"abrigo\"', '+', '0.282*\"iguaizinho\"', '+', '0.189*\"bastioes\"', '+', '0.134*\"enviado\"', '+', '0.095*\"gabinetes\"', '+', '-0.075*\"inhuman\"', '+', '0.074*\"decoro\"', '+', '0.066*\"inhame\"', '+', '0.061*\"oficio\"']\n",
      "Tópico 2:\n",
      "['0.619*\"iguaizinho\"', '+', '0.455*\"bastioes\"', '+', '-0.365*\"portr\"', '+', '0.261*\"enviado\"', '+', '-0.175*\"gabinetes\"', '+', '-0.166*\"inhuman\"', '+', '0.090*\"inhame\"', '+', '-0.089*\"abominavel\"', '+', '-0.086*\"moratoria\"', '+', '-0.082*\"transfiram\"']\n",
      "Tópico 3:\n",
      "['-0.680*\"gabinetes\"', '+', '-0.345*\"abominavel\"', '+', '0.318*\"portr\"', '+', '-0.306*\"these\"', '+', '-0.234*\"inhuman\"', '+', '0.218*\"abrigo\"', '+', '-0.137*\"enviado\"', '+', '-0.124*\"iguaizinho\"', '+', '-0.100*\"etc\"', '+', '-0.093*\"bastioes\"']\n",
      "Tópico 4:\n",
      "['0.896*\"inhuman\"', '+', '-0.275*\"gabinetes\"', '+', '-0.144*\"these\"', '+', '-0.119*\"abrigo\"', '+', '0.114*\"news\"', '+', '0.090*\"iguaizinho\"', '+', '-0.089*\"abominavel\"', '+', '-0.077*\"pedagio\"', '+', '0.071*\"bastioes\"', '+', '-0.064*\"lastima\"']\n",
      "Tópico 5:\n",
      "['-0.309*\"enviado\"', '+', '-0.308*\"etc\"', '+', '-0.293*\"transfiram\"', '+', '0.293*\"gabinetes\"', '+', '-0.275*\"moratoria\"', '+', '0.267*\"abrigo\"', '+', '-0.220*\"caverno\"', '+', '0.204*\"portr\"', '+', '-0.197*\"daninha\"', '+', '-0.194*\"dgsaude\"']\n",
      "Tópico 6:\n",
      "['-0.731*\"enviado\"', '+', '0.315*\"transfiram\"', '+', '0.252*\"iguaizinho\"', '+', '-0.246*\"liverpol\"', '+', '0.184*\"chicungunya\"', '+', '0.181*\"bastioes\"', '+', '0.144*\"moratoria\"', '+', '-0.108*\"alfonso\"', '+', '0.108*\"decoro\"', '+', '0.108*\"etc\"']\n",
      "Tópico 7:\n",
      "['0.629*\"moratoria\"', '+', '-0.336*\"transfiram\"', '+', '-0.236*\"caverno\"', '+', '-0.216*\"enviado\"', '+', '-0.211*\"covidportugal\"', '+', '-0.207*\"pedagio\"', '+', '0.206*\"liverpol\"', '+', '0.194*\"dgsaude\"', '+', '-0.189*\"lastima\"', '+', '-0.177*\"certame\"']\n",
      "Tópico 8:\n",
      "['0.527*\"etc\"', '+', '-0.504*\"covidportugal\"', '+', '-0.466*\"caverno\"', '+', '-0.245*\"moratoria\"', '+', '0.163*\"certame\"', '+', '0.157*\"transfiram\"', '+', '-0.138*\"liverpol\"', '+', '0.123*\"news\"', '+', '0.092*\"birmingham\"', '+', '0.078*\"compartilhamo\"']\n",
      "Tópico 9:\n",
      "['-0.689*\"dgsaude\"', '+', '0.425*\"moratoria\"', '+', '0.204*\"etc\"', '+', '-0.176*\"catolicar\"', '+', '0.174*\"pedagio\"', '+', '0.172*\"certame\"', '+', '-0.167*\"covidportugal\"', '+', '0.166*\"enviado\"', '+', '0.159*\"lastima\"', '+', '-0.132*\"daninha\"']\n"
     ]
    }
   ],
   "source": [
    "for topic_num, topic_words in lsa_model.print_topics(num_topics=num_topics):\n",
    "    print('Tópico {}:'.format(topic_num))\n",
    "    words = topic_words.split()\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9841ef-34da-46d8-962d-a79c158c0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a perplexidade\n",
    "#perplexity = gensim.models.CoherenceModel(topics=[lsa_model.show_topic(i) for i in range(num_topics)], corpus=reconstructed_corpus, dictionary=dictionary, coherence='u_mass').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c853cc3-aa2b-47c0-846e-a14c4bbe32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém a matriz de tópicos\n",
    "topic_matrix = lsa_model.get_topics()\n",
    "topic_matrix = topic_matrix.T\n",
    "\n",
    "# Calcula a distância de cosseno entre todos os pares de tópicos\n",
    "distance_matrix = cosine_distances(topic_matrix)\n",
    "\n",
    "# Calcula a mediana da matriz de distância de tópicos\n",
    "median_distance = np.median(distance_matrix)\n",
    "\n",
    "# Imprime a mediana da matriz de distância de tópicos\n",
    "print(\"A distância de tópicos: \", median_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e51c518-2f3e-4022-9df7-f3521945a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto TfidfModel a partir do corpus\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "# Calcula a matriz TF-IDF para o corpus\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# Calcula a divergência KL simétrica entre todos os pares de tópicos\n",
    "similarity_matrix = MatrixSimilarity(lsa_model[corpus_tfidf], num_features=num_topics)\n",
    "kl_divergences = similarity_matrix.get_similarities(topic_matrix)\n",
    "\n",
    "# Substitui NaNs por zeros\n",
    "#kl_divergences = np.nan_to_num(kl_divergences)\n",
    "\n",
    "# Calcula a média da matriz de divergência KL simétrica\n",
    "mean_kl_divergence = np.mean(kl_divergences)\n",
    "\n",
    "# Imprime a média da matriz de divergência KL simétrica\n",
    "print(\"Média da matriz de diverência KL simétrica: \", mean_kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b407af70-8a60-4b84-8081-120e89b6505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coerência C_umass é:  -4.120386072467498\n"
     ]
    }
   ],
   "source": [
    "# Cria um objeto CoherenceModel para calcular a coerência C_umass\n",
    "coherence_model = CoherenceModel(model=lsa_model, corpus=corpus_tfidf, dictionary=dictionary, coherence='u_mass')\n",
    "\n",
    "# Calcula a coerência C_umass\n",
    "umass_coherence = coherence_model.get_coherence()\n",
    "\n",
    "print(\"A coerência C_umass é: \", umass_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a99e3e1f-80b3-4371-bceb-81d2763232c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coerência C_w2v é:  0.90548134\n"
     ]
    }
   ],
   "source": [
    "textos_preprocessados = []\n",
    "for texto in docs:\n",
    "    # Tokenização das palavras\n",
    "    tokens = word_tokenize(texto)\n",
    "    textos_preprocessados.append(tokens)\n",
    "\n",
    "# Cria um objeto CoherenceModel para calcular a coerência C_w2v\n",
    "coherence_model = CoherenceModel(model=lsa_model, texts=textos_preprocessados, dictionary=dictionary, coherence='c_w2v', window_size=10)\n",
    "\n",
    "# Calcula a coerência C_w2v\n",
    "w2v_coherence = coherence_model.get_coherence()\n",
    "\n",
    "print(\"A coerência C_w2v é: \", w2v_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d7313-fcb7-498a-8af7-c17ccf658857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
