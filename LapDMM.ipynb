{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113865ef-d802-4ed9-8dd4-5f0fd1089093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719185e6-6ec0-4fca-9742-c2434dadabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde2f7aa-e486-4b90-9f1a-d04db098768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(df, text_column):\n",
    "    df['tokens'] = df[text_column].apply(preprocess_and_tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c842a907-275f-4784-b8ad-be8220f55997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_and_dictionary(df, text_column):\n",
    "    documents = df[text_column].tolist()\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b2362a-5753-45fd-950e-f5dc0811b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(doc1, doc2, dictionary):\n",
    "    vec1 = np.zeros(len(dictionary))\n",
    "    vec2 = np.zeros(len(dictionary))\n",
    "\n",
    "    for idx, freq in doc1:\n",
    "        vec1[idx] = freq\n",
    "\n",
    "    for idx, freq in doc2:\n",
    "        vec2[idx] = freq\n",
    "\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9595a08a-0b40-423f-b483-28320f51cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_row_similarity(i, corpus, dictionary, similarity_threshold):\n",
    "    num_docs = len(corpus)\n",
    "    row = np.zeros(num_docs)\n",
    "\n",
    "    for j in range(i, num_docs):\n",
    "        if i == j:\n",
    "            row[j] = 1\n",
    "        else:\n",
    "            similarity = compute_cosine_similarity(corpus[i], corpus[j], dictionary)\n",
    "            if similarity >= similarity_threshold:\n",
    "                row[j] = -similarity\n",
    "\n",
    "    row[i] = -row.sum()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba4706d-325b-4069-b9ee-899c1129b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian_matrix(corpus, dictionary, similarity_threshold=0.9):\n",
    "    num_docs = len(corpus)\n",
    "    laplacian = lil_matrix((num_docs, num_docs))\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        rows = list(executor.map(lambda i: compute_row_similarity(i, corpus, dictionary, similarity_threshold), range(num_docs)))\n",
    "\n",
    "    laplacian = np.array(rows)\n",
    "    laplacian_sparse = csr_matrix(laplacian)\n",
    "\n",
    "    return laplacian_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f6cbab-f921-421d-8abe-0edb6545d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_dmm(df, text_column, num_topics, similarity_threshold=0.9):\n",
    "    corpus, dictionary = create_corpus_and_dictionary(df, text_column)\n",
    "    laplacian = compute_laplacian_matrix(corpus, dictionary, similarity_threshold)\n",
    "    laplacian_eigenvalues, laplacian_eigenvectors = eigsh(laplacian, k=num_topics, which='SM')\n",
    "\n",
    "    eigenvectors_to_use = laplacian_eigenvectors[:, :num_topics]\n",
    "    document_topic_distribution = eigenvectors_to_use / eigenvectors_to_use.sum(axis=1)[:, None]\n",
    "\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "\n",
    "    return lda_model, document_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7c7e4-bcc1-4e4e-8530-3a0cccd1435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fce986-ce30-4554-acc8-9dedfa3da3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize_dataframe(df, 'texto')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706f62d5-3311-4cf0-ae6b-badee60e2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "lda_model, document_topic_distribution = laplacian_dmm(df, 'tokens', num_topics)\n",
    "\n",
    "# Print the topic-word distribution\n",
    "for topic_id in range(num_topics):\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(lda_model.show_topic(topic_id))\n",
    "\n",
    "# Print the document-topic distribution\n",
    "for doc_id, doc_topic_dist in enumerate(document_topic_distribution):\n",
    "    print(f\"Document {doc_id}: Topic distribution: {doc_topic_dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
