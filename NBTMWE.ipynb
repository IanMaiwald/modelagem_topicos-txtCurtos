{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25e6a1a-8ba7-4a79-9ea6-bb7dcf60e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beec515d-cfbc-4659-b0a5-2a08995e00ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBTMWE:\n",
    "    def __init__(self, num_topics, alpha, beta, num_iterations):\n",
    "        self.num_topics = num_topics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def preprocess(self, df, column_name):\n",
    "        # Pré-processar o texto\n",
    "        processed_docs = []\n",
    "\n",
    "        for text in df[column_name]:\n",
    "            # Tokenizar e remover stopwords\n",
    "            tokens = word_tokenize(text)\n",
    "            filtered_tokens = [token for token in tokens]\n",
    "            processed_docs.append(filtered_tokens)\n",
    "\n",
    "        return processed_docs\n",
    "\n",
    "    def train_embeddings(self, tokenized_documents, size=100, window=5, min_count=1):\n",
    "        # Treinar os embeddings das palavras\n",
    "        self.word2vec = Word2Vec(sentences=tokenized_documents, size=size, window=window, min_count=min_count, workers=4)\n",
    "        self.embedding_matrix = self.word2vec.wv.vectors\n",
    "\n",
    "    def build_vocabulary(self, tokenized_documents):\n",
    "        # Construir o vocabulário\n",
    "        self.vectorizer = CountVectorizer(analyzer=lambda x: x, min_df=1)\n",
    "        self.vectorizer.fit(tokenized_documents)\n",
    "        self.word_index = self.vectorizer.vocabulary_\n",
    "\n",
    "    def initialize_topic_assignments(self):\n",
    "        self.topic_word_count = np.zeros((self.num_topics, len(self.word_index)))\n",
    "        self.topic_count = np.zeros(self.num_topics)\n",
    "        self.biterm_topic_assignments = []\n",
    "\n",
    "        for doc in self.biterms:\n",
    "            biterm_topic = []\n",
    "            for w1, w2 in doc:\n",
    "                # Converter palavras em índices numéricos\n",
    "                w1_idx, w2_idx = self.word_index[w1], self.word_index[w2]\n",
    "\n",
    "                # Atribuir tópicos aleatoriamente aos bitermos\n",
    "                topic = np.random.randint(self.num_topics)\n",
    "                biterm_topic.append(topic)\n",
    "\n",
    "                # Atualizar contagens\n",
    "                self.topic_word_count[topic, w1_idx] += 1\n",
    "                self.topic_word_count[topic, w2_idx] += 1\n",
    "                self.topic_count[topic] += 1\n",
    "            self.biterm_topic_assignments.append(biterm_topic)\n",
    "\n",
    "    def gibbs_sampling(self):\n",
    "        for iter in range(self.num_iterations):\n",
    "            for doc_idx, doc in enumerate(self.biterms):\n",
    "                for biterm_idx, (w1, w2) in enumerate(doc):\n",
    "                    # Converter palavras em índices numéricos\n",
    "                    w1_idx, w2_idx = self.word_index[w1], self.word_index[w2]\n",
    "\n",
    "                    # Remover atribuições de tópicos atuais\n",
    "                    current_topic = self.biterm_topic_assignments[doc_idx][biterm_idx]\n",
    "                    self.topic_word_count[current_topic, w1_idx] -= 1\n",
    "                    self.topic_word_count[current_topic, w2_idx] -= 1\n",
    "                    self.topic_count[current_topic] -= 1\n",
    "\n",
    "                    # Calcular probabilidades do tópico\n",
    "                    probabilities = (self.topic_word_count[:, w1_idx] + self.beta) * (self.topic_word_count[:, w2_idx] + self.beta) / ((self.topic_count + len(self.word_index) * self.beta) ** 2)\n",
    "\n",
    "                    # Normalizar probabilidades\n",
    "                    probabilities /= np.sum(probabilities)\n",
    "\n",
    "                    # Amostrar novo tópico\n",
    "                    new_topic = np.random.choice(self.num_topics, p=probabilities)\n",
    "\n",
    "                    # Atualizar atribuições de tópicos e contagens\n",
    "                    self.biterm_topic_assignments[doc_idx][biterm_idx] = new_topic\n",
    "                    self.topic_word_count[new_topic, w1_idx] += 1\n",
    "                    self.topic_word_count[new_topic, w2_idx] += 1\n",
    "                    self.topic_count[new_topic] += 1\n",
    "\n",
    "                    \n",
    "    def fit(self, df, column_name):\n",
    "        tokenized_documents = self.preprocess(df, column_name)\n",
    "        self.train_embeddings(tokenized_documents)\n",
    "        self.build_vocabulary(tokenized_documents)\n",
    "\n",
    "        self.biterms = []\n",
    "        for doc in tokenized_documents:\n",
    "            biterms_in_doc = [(doc[i], doc[j]) for i in range(len(doc)) for j in range(i + 1, len(doc))]\n",
    "            self.biterms.append(biterms_in_doc)\n",
    "\n",
    "        self.initialize_topic_assignments()\n",
    "        self.gibbs_sampling()\n",
    "\n",
    "    def get_topics(self, num_words=10):\n",
    "        topics = []\n",
    "        for topic_idx in range(self.num_topics):\n",
    "            top_word_indices = self.topic_word_count[topic_idx].argsort()[-num_words:][::-1]\n",
    "            top_words = [self.vectorizer.get_feature_names()[i] for i in top_word_indices]\n",
    "            topics.append(top_words)\n",
    "\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83f977-36c7-41cf-85ae-f436689c8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae13a5-b3be-4794-86a2-54c796b2670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbtmwe = NBTMWE(num_topics=10, alpha=1, beta=0.01, num_iterations=50)\n",
    "nbtmwe.fit(df, 'texto')\n",
    "topics = nbtmwe.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e2459-c1cc-425b-8eb1-d877d8a755e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in enumerate(topics):\n",
    "    print(f\"Topic {idx + 1}: {', '.join(topic)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
