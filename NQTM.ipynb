{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4296f4d-1674-4b7b-9b27-83b7e762d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7589fa7f-5e46-49a1-92bd-e09ce05f751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NQTM:\n",
    "    def __init__(self, n_topics, n_samples, max_iter=100, tol=1e-4):\n",
    "        self.n_topics = n_topics\n",
    "        self.n_samples = n_samples\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.Q = None\n",
    "\n",
    "    def _negative_sampling(self, P):\n",
    "        neg_P = np.zeros_like(P)\n",
    "        for t in range(self.n_topics):\n",
    "            sorted_indices = np.argsort(-P[:, t])\n",
    "            neg_indices = sorted_indices[self.n_samples:]\n",
    "            neg_P[neg_indices, t] = P[neg_indices, t]\n",
    "        return neg_P\n",
    "\n",
    "    def _quantization(self, P):\n",
    "        return (P > 0).astype(float)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        n_docs, n_words = X.shape\n",
    "        \n",
    "        # Inicializar a matriz P e a matriz Q.\n",
    "        P = np.random.rand(n_docs, self.n_topics)\n",
    "        P /= P.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.Q = np.random.rand(self.n_topics, n_words)\n",
    "        self.Q /= self.Q.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            P_old = P.copy()\n",
    "\n",
    "            # E-step\n",
    "            P = X @ self.Q.T\n",
    "            P = normalize(P, axis=1, norm='l1')\n",
    "\n",
    "            # M-step\n",
    "            neg_P = self._negative_sampling(P)\n",
    "            self.Q = normalize((X.T @ (P - neg_P)).T, axis=1, norm='l1')\n",
    "            self.Q = self._quantization(self.Q)\n",
    "\n",
    "            if np.linalg.norm(P - P_old) < self.tol:\n",
    "                break\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83de4e4b-6391-46f3-bb3b-8116d620b816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coronavirus aceitar braco abrir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>achar eleitor bolsonaro medo coronavirus febre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fome coronavirus entrar fila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trancar replies twetr sala coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caso coronavirus confirmar Brasil mundo querer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>preciso comando vermelho decretar toque recolh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>urgente reporter confirmar segundo morte coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>informativo elaborar equipe viano azevedo advo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>vivo Paulo confirmar primeiro morte covid19 Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>bolsonaro protesto coronavirus pegar gado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    texto\n",
       "0                         coronavirus aceitar braco abrir\n",
       "1       achar eleitor bolsonaro medo coronavirus febre...\n",
       "2                            fome coronavirus entrar fila\n",
       "3                  trancar replies twetr sala coronavirus\n",
       "4       caso coronavirus confirmar Brasil mundo querer...\n",
       "...                                                   ...\n",
       "99996   preciso comando vermelho decretar toque recolh...\n",
       "99997   urgente reporter confirmar segundo morte coron...\n",
       "99998   informativo elaborar equipe viano azevedo advo...\n",
       "99999   vivo Paulo confirmar primeiro morte covid19 Br...\n",
       "100000          bolsonaro protesto coronavirus pegar gado\n",
       "\n",
       "[100000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825b8a45-09e1-4f17-b8ca-50362e08e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df[\"texto\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0216ba-0286-4a6b-bd3b-677589320a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorizar os documentos\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=None)\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34bc99-1b73-41e5-b8ec-dc720e3f68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo NQTM\n",
    "n_topics = 10\n",
    "n_samples = 10\n",
    "nqtm = NQTM(n_topics, n_samples)\n",
    "topic_matrix = nqtm.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbbf5f-4f2a-4eb7-b959-68858084894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir os tópicos\n",
    "topic_word_matrix = nqtm.Q\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(topic_word_matrix):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
