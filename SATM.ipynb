{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a24837-8d52-45cf-83ab-5872fd94d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542e2f57-0f73-4f81-abe7-78d60f8b813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76302273-ad87-47df-b24b-3afde4a8cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges_in_chunks(graph, tfidf_matrix, chunk_size=1000, threshold=0.2):\n",
    "    n_docs = tfidf_matrix.shape[0]\n",
    "    for i in range(0, n_docs, chunk_size):\n",
    "        for j in range(i, n_docs, chunk_size):\n",
    "            i_chunk_start, i_chunk_end = i, min(i + chunk_size, n_docs)\n",
    "            j_chunk_start, j_chunk_end = j, min(j + chunk_size, n_docs)\n",
    "\n",
    "            # Calcular similaridades para o bloco atual\n",
    "            similarities_chunk = cosine_similarity(tfidf_matrix[i_chunk_start:i_chunk_end], tfidf_matrix[j_chunk_start:j_chunk_end])\n",
    "\n",
    "            # Adicionar arestas no grafo com base na similaridade do cosseno\n",
    "            for i_idx, j_idx in itertools.product(range(similarities_chunk.shape[0]), range(similarities_chunk.shape[1])):\n",
    "                similarity = similarities_chunk[i_idx, j_idx]\n",
    "\n",
    "                # Ignorar similaridades na diagonal principal quando i == j\n",
    "                if i == j and i_idx == j_idx:\n",
    "                    continue\n",
    "\n",
    "                if similarity > threshold:\n",
    "                    graph.add_edge(i_chunk_start + i_idx, j_chunk_start + j_idx, weight=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8624ea6c-2e8b-49ca-a0a6-18e0eefba643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura dos dados do dataframe\n",
    "#df = pd.read_csv('datasets/(processado-final)textos_tuitesPt_2020.csv.gz', names=['texto'])\n",
    "df = pd.read_csv('datasets/(processado)textos_tuitesPt_2020_0.csv', names=['texto'])\n",
    "\n",
    "# Elimina um valor flutuante que aparece no dataframe (por razões misteriosas)\n",
    "# o algoritmo não aceita o valor flutuante, que precisa ser filtrado\n",
    "df = df[df['texto'].apply(lambda x: isinstance(x, str))]\n",
    "df['texto'].apply(type).value_counts()\n",
    "\n",
    "df\n",
    "\n",
    "df['tokens'] = df['texto'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9998ac7b-4ba2-4580-a85e-cfcc54eac453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vetorização usando TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize, stop_words=None, max_df=0.9, min_df=0.1)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64fe154-41da-4fd6-b9e0-9e2c6e90b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o grafo de documentos\n",
    "graph = nx.Graph()\n",
    "graph.add_nodes_from(df.index)\n",
    "add_edges_in_chunks(graph, tfidf_matrix, chunk_size=1000, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893ab7c-b485-4f66-bd80-5ac568ebc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregação de documentos\n",
    "aggregated_docs = []\n",
    "while graph.number_of_nodes() > 0:\n",
    "    # Encontre o nó com o maior grau\n",
    "    max_degree_node = max(graph.nodes, key=graph.degree)\n",
    "\n",
    "    # Encontre todos os vizinhos do nó de maior grau\n",
    "    neighbors = list(graph.neighbors(max_degree_node))\n",
    "\n",
    "    # Combine os documentos do nó de maior grau e seus vizinhos\n",
    "    aggregated_doc = []\n",
    "    for node in neighbors + [max_degree_node]:\n",
    "        aggregated_doc.extend(data.loc[node, 'tokens'])\n",
    "        graph.remove_node(node)\n",
    "\n",
    "    aggregated_docs.append(aggregated_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f07ac5-6813-48d6-b161-58972b479340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelagem de tópicos usando LDA\n",
    "dictionary = corpora.Dictionary(aggregated_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in aggregated_docs]\n",
    "\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6a6b1-c33a-4364-ad05-6f859d4f382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir tópicos\n",
    "for i, topic in lda_model.print_topics(num_topics=10, num_words=5):\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
